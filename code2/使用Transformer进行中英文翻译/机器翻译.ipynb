{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6016f24",
   "metadata": {},
   "source": [
    "## 1 导入相关包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af03343c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch\n",
    "from nltk.tokenize import word_tokenize\n",
    "import jieba\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f5dbfd",
   "metadata": {},
   "source": [
    "## 2 定义dataset类\n",
    "用于传入dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d07e8aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TranslationDataset(Dataset):\n",
    "    # 创建数据集\n",
    "    def __init__(self, src, tgt):\n",
    "        \"\"\"\n",
    "        初始化\n",
    "        :param src: 源数据(经tokenizer处理后)\n",
    "        :param tgt: 目标数据(经tokenizer处理后)\n",
    "        \"\"\"\n",
    "        self.src = src\n",
    "        self.tgt = tgt\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.src[i], self.tgt[i]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dab7e16",
   "metadata": {},
   "source": [
    "## 3 定义tokenizer类\n",
    "其作用是读取源数据并将其处理成可供模型输入的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6744d89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    ## 定义tokenizer,对原始数据进行处理\n",
    "    def __init__(self, en_path, ch_path, count_min=5):\n",
    "        \"\"\"\n",
    "        初始化\n",
    "        :param en_path: 英文数据路径\n",
    "        :param ch_path: 中文数据路径\n",
    "        :param count_min: 对出现次数少于这个次数的数据进行过滤\n",
    "        \"\"\"\n",
    "        self.en_path = en_path  # 英文路径\n",
    "        self.ch_path = ch_path  # 中文路径\n",
    "        self.__count_min = count_min  # 对出现次数少于这个次数的数据进行过滤\n",
    "\n",
    "        # 读取原始英文数据\n",
    "        self.en_data = self.__read_ori_data(en_path)\n",
    "        # 读取原始中文数据\n",
    "        self.ch_data = self.__read_ori_data(ch_path)\n",
    "        # 英文index_2_word\n",
    "        self.en_index_2_word = ['0']\n",
    "        # 中文index_2_word\n",
    "        self.ch_index_2_word = ['0']\n",
    "        # 英文word_2_index\n",
    "        self.en_word_2_index = {'unK': 0}\n",
    "        # 中文word_2_index\n",
    "        self.ch_word_2_index = {'unK': 0}\n",
    "        # 中英文字符计数\n",
    "        self.__en_count = {'<pad>': count_min}\n",
    "        self.__ch_count = {'<pad>': count_min, '<bos>': count_min, '<eos>': count_min}\n",
    "        # 批量tokenize数据的开始位置与结束位置，即每次tokenize 10000组数据\n",
    "        self.__start = 0\n",
    "        self.__end = 10000\n",
    "        # 创建英文词汇表\n",
    "        self.__build_vocab()\n",
    "\n",
    "    def __read_ori_data(self, path):\n",
    "        \"\"\"\n",
    "        读取原始数据\n",
    "        :param path: 数据路径\n",
    "        :return: 返回一个列表，每个元素是一条数据\n",
    "        \"\"\"\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            data = f.read().split('\\n')[:-1]\n",
    "        return data\n",
    "\n",
    "    def __en_token_count(self):\n",
    "        \"\"\"\n",
    "        英文token计数\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        x = 0\n",
    "        # 对英文数据进行遍历\n",
    "        for sentence in self.en_data:\n",
    "            x += 1\n",
    "            if x % 10000 == 0:\n",
    "                print('英文词汇统计进度 ', x / len(self.en_data))\n",
    "            # 对句子进行分词\n",
    "            words = word_tokenize(sentence)\n",
    "            # 对分词后的结果进行计数\n",
    "            for word in words:\n",
    "                if word not in self.__en_count:\n",
    "                    self.__en_count[word] = 1\n",
    "                else:\n",
    "                    self.__en_count[word] += 1\n",
    "\n",
    "    def __ch_token_count(self):\n",
    "        \"\"\"\n",
    "        中文token计数\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        x = 0\n",
    "        #  对中文数据进行遍历\n",
    "        for sentence in self.ch_data:\n",
    "            x += 1\n",
    "            if x % 10000 == 0:\n",
    "                print('中文词汇统计进度 ', x / len(self.ch_data))\n",
    "            #  对句子进行分词\n",
    "            words = jieba.cut(sentence)\n",
    "            #   对分词后的结果进行计数\n",
    "            for word in words:\n",
    "                if word not in self.__ch_count:\n",
    "                    self.__ch_count[word] = 1\n",
    "                else:\n",
    "                    self.__ch_count[word] += 1\n",
    "\n",
    "    def __build_vocab(self):\n",
    "        \"\"\"\n",
    "        构建词汇表\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # 英文token计数\n",
    "        self.__en_token_count()\n",
    "        # 中文token计数\n",
    "        self.__ch_token_count()\n",
    "        # 对英文数据的键值对进行遍历\n",
    "        for word, count in self.__en_count.items():\n",
    "            # 如果出现次数大于等于最小限度\n",
    "            if count >= self.__count_min:\n",
    "                # 加入词表\n",
    "                self.en_word_2_index[word] = len(self.en_index_2_word)\n",
    "                self.en_index_2_word.append(word)\n",
    "            # 否则将这个词对应的index设为0，即指向unk,即当句子中出现时，用unk填充\n",
    "            else:\n",
    "                self.en_word_2_index[word] = 0\n",
    "        self.en_word_2_index.update({'<pad>': len(self.en_index_2_word)})\n",
    "        self.en_index_2_word.append('<pad>')\n",
    "\n",
    "        # 释放英文字符的空间\n",
    "        self.__en_count = None\n",
    "        # 对中文数据的键值对进行遍历，以下操作与上面相同\n",
    "        for word, count in self.__ch_count.items():\n",
    "            if count >= self.__count_min:\n",
    "                self.ch_word_2_index[word] = len(self.ch_index_2_word)\n",
    "                self.ch_index_2_word.append(word)\n",
    "            else:\n",
    "                self.ch_word_2_index[word] = 0\n",
    "        self.__ch_count = None\n",
    "        self.ch_word_2_index.update({'<pad>': len(self.ch_index_2_word), '<bos>': len(self.ch_index_2_word) + 1,\n",
    "                                     '<eos>': len(self.ch_index_2_word) + 2})\n",
    "        self.ch_index_2_word.append('<pad>')\n",
    "        self.ch_index_2_word.append('<bos>')\n",
    "        self.ch_index_2_word.append('<eos>')\n",
    "\n",
    "    def split_data(self, data, func):\n",
    "        data_type = type(data)  # 检测数据类型\n",
    "        # 判断数据是不是字符串，如果是则放到列表以内\n",
    "        if data_type == str:\n",
    "            data = [data]\n",
    "        # 用于存储编码后的数据\n",
    "        tokens_data = []\n",
    "        # 对数据进行遍历\n",
    "        for sentence in data:\n",
    "            # 对数据进行分词\n",
    "            tokens = func(sentence)\n",
    "            tokens_data.append(list(tokens))\n",
    "        return tokens_data\n",
    "\n",
    "    def en_encode(self, data):\n",
    "        \"\"\"\n",
    "        英文数据编码\n",
    "        :param data: 需要编码的数据\n",
    "        :return: 返回编码后的数据集\n",
    "        \"\"\"\n",
    "        src = self.split_data(data, word_tokenize)\n",
    "        tokenized_data = []\n",
    "        for sentence in src:\n",
    "            # 用于存放每个句子对应的编码\n",
    "            en_tokens = []\n",
    "            # 对分词结果进行遍历\n",
    "            for i in sentence:\n",
    "                # 对于结果进行编码\n",
    "                en_tokens.append(self.en_word_2_index.get(i, 0))\n",
    "            tokenized_data.append(en_tokens)\n",
    "            # 返回编码后的数据\n",
    "        return tokenized_data\n",
    "\n",
    "    def decode(self, data):\n",
    "        \"\"\"\n",
    "        数据解码\n",
    "        :param data: 这里传入一个中文的index\n",
    "        :return: 返回解码后的一个字符\n",
    "        \"\"\"\n",
    "        return self.ch_index_2_word[data]\n",
    "\n",
    "    def ch_encode(self, data):\n",
    "        \"\"\"\n",
    "        中文编码\n",
    "        :param data: 需要编码的数据\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        tgt = self.split_data(data, jieba.cut)\n",
    "        # 用于存储编码后的数据\n",
    "        tokenized_data = []\n",
    "        # 对数据进行遍历\n",
    "        for sentence in tgt:\n",
    "            # 用于存放每个句子对应的编码\n",
    "            ch_tokens = []\n",
    "            # 对分词结果进行遍历\n",
    "            for i in sentence:\n",
    "                # 编码\n",
    "                ch_tokens.append(self.ch_word_2_index.get(i, 0))\n",
    "            tokenized_data.append(ch_tokens)\n",
    "        # 返回编码后的数据\n",
    "        return tokenized_data\n",
    "\n",
    "    def __get_datasets(self):\n",
    "        \"\"\"\n",
    "        获取数据集\n",
    "        :return:返回DataSet类型的数据 或者 None\n",
    "        \"\"\"\n",
    "        # 获取一部分数据，这是一个生成器\n",
    "        src, tgt = next(self.__data_generator__())\n",
    "        # 将下一个数据切分位置向后偏移10000\n",
    "        self.__start += 10000\n",
    "        self.__end += 10000\n",
    "        # 如果返回空列表或者None，返回None\n",
    "        if src == [] or src == None:\n",
    "            return None\n",
    "        # 将数据编码并\n",
    "        src = self.en_encode(src)\n",
    "        tgt = self.ch_encode(tgt)\n",
    "        # 返回数据集\n",
    "        return TranslationDataset(src, tgt)\n",
    "\n",
    "    def another_process(self, batch_datas):\n",
    "        \"\"\"\n",
    "        特殊处理，这里传入一个batch的数据，并对这个batch的数据进行填充，使得每一行的数据长度相同。这里填充pad 空字符  bos 开始  eos结束\n",
    "        :param batch_datas: 一个batch的数据\n",
    "        :return: 返回填充后的数据\n",
    "        \"\"\"\n",
    "        # 创建四个空字典存储数据\n",
    "        en_index, ch_index = [], []  # 中文英文索引，中文索引\n",
    "        en_len, ch_len = [], []  # 没行英文长度，每行中文长度\n",
    "\n",
    "        for en, ch in batch_datas:  # 对batch进行遍历，将所有数据的索引与长度加入四个列表\n",
    "            en_index.append(en)\n",
    "            ch_index.append(ch)\n",
    "            en_len.append(len(en))\n",
    "            ch_len.append(len(ch))\n",
    "\n",
    "        # 获取中英文的最大长度，根据这个长度对所有数据进行填充，使每行数据长度相同\n",
    "        max_en_len = max(en_len)\n",
    "        max_ch_len = max(ch_len)\n",
    "        max_len = max(max_en_len, max_ch_len + 2)\n",
    "\n",
    "        # 英文数据填充，i是原始数据，后面是填充的pad\n",
    "        en_index = [i + [self.en_word_2_index['<pad>']] * (max_len - len(i)) for i in en_index]\n",
    "        # 中文数据填充 先填充bos表示句子开始，后面接原始数据，最后填充eos表示句子结束，后面接pad\n",
    "        ch_index = [[self.ch_word_2_index['<bos>']] + i + [self.ch_word_2_index['<eos>']] +\n",
    "                    [self.ch_word_2_index['<pad>']] * (max_len - len(i) + 1) for i in ch_index]\n",
    "\n",
    "        # 将处理后的数据转换为tensor并放到相应设备上\n",
    "        en_index = torch.tensor(en_index)\n",
    "        ch_index = torch.tensor(ch_index)\n",
    "        return en_index, ch_index\n",
    "\n",
    "    def get_dataloader(self, batch_size=32):\n",
    "        \"\"\"\n",
    "        获取dataloader\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # 获取数据集\n",
    "        data = self.__get_datasets()\n",
    "        p = 0\n",
    "        # 如果数据集为空，返回None\n",
    "        if data is None:\n",
    "            self.__start = 0\n",
    "            self.__end = 10000\n",
    "            data = self.__get_datasets()\n",
    "            p = 1\n",
    "        # 返回DataLoader类型的数据\n",
    "        return DataLoader(data, batch_size=batch_size, shuffle=True, collate_fn=self.another_process), p\n",
    "\n",
    "    def __data_generator__(self):\n",
    "        # 数据集生成器\n",
    "        while True:\n",
    "            # 如果结束位置大于数据集长度，返回剩下的所有数据\n",
    "            while self.__end > len(self.en_data):\n",
    "                self.__end = len(self.en_data)\n",
    "                yield self.en_data[self.__start:self.__end], self.ch_data[self.__start:self.__end]\n",
    "            # 返回start 到 end之间的源数据\n",
    "            yield self.en_data[self.__start:self.__end], self.ch_data[self.__start:self.__end]\n",
    "\n",
    "    # 获取英文词表大小\n",
    "    def get_en_vocab_size(self):\n",
    "        return len(self.en_index_2_word)\n",
    "\n",
    "    # 获取中文词表大小\n",
    "    def get_ch_vocab_size(self):\n",
    "        return len(self.ch_index_2_word)\n",
    "\n",
    "    # 获取数据集大小\n",
    "    def get_dataset_size(self):\n",
    "        return len(self.en_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f6d085",
   "metadata": {},
   "source": [
    "## 4 定义batch类\n",
    "其作用是生成掩码与统计非填充字符数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f84ee93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batch:\n",
    "    # 批次类,对每一个批次的数据进行掩码生成操作\n",
    "    def __init__(self, src, trg=None, tokenizer=None, device='cuda'):\n",
    "        \"\"\"\n",
    "        初始化函数\n",
    "        :param src: 源数据\n",
    "        :param trg: 目标数据\n",
    "        :param tokenizer: 分词器\n",
    "        :param device: 训练设备\n",
    "        \"\"\"\n",
    "        # 将输入、输出单词id表示的数据规范成整数类型并转换到训练设备上\n",
    "        src = src.to(device).long()\n",
    "        trg = trg.to(device).long()\n",
    "        self.src = src  # 源数据 (batch, seq_len)\n",
    "        self.__pad = tokenizer.ch_word_2_index['<pad>']  # 填充字符的索引\n",
    "        # 对于当前输入的语句非空部分进行判断，这里是对源数据进行掩码操作，将填充的内容置为0\n",
    "        # 并在seq length前面增加一维，形成维度为 1×seq length 的矩阵\n",
    "        self.src_mask = (src != self.__pad).unsqueeze(-2)\n",
    "        # 如果输出目标不为空，则需要对解码器使用的目标语句进行掩码\n",
    "        if trg is not None:\n",
    "            # 解码器使用的目标输入部分\n",
    "            self.trg = trg[:, : -1]\n",
    "            # 解码器训练时应预测输出的目标结果\n",
    "            self.trg_y = trg[:, 1:]\n",
    "            # 将目标输入部分进行注意力掩码\n",
    "            self.trg_mask = self.make_std_mask(self.trg, self.__pad)\n",
    "            # 将应输出的目标结果中实际的词数进行统计\n",
    "            self.ntokens = (self.trg_y != self.__pad).data.sum()\n",
    "\n",
    "    # 掩码操作\n",
    "    @staticmethod\n",
    "    def make_std_mask(tgt, pad):\n",
    "        \"\"\"\n",
    "        生成掩码矩阵\n",
    "        :param tgt: 目标数据\n",
    "        :param pad: 填充字符的索引\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        tgt_mask = (tgt != pad).unsqueeze(-2)  # 首先对pad进行掩码生成\n",
    "        # 这里对注意力进行掩码操作并与pad掩码结合起来。\n",
    "        tgt_mask = tgt_mask & Variable(subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))\n",
    "        return tgt_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4b2266",
   "metadata": {},
   "source": [
    "#### 注意力掩码生成函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26bfeea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"\"\"\n",
    "    注意力机制掩码生成\n",
    "    :param size: 句子长度\n",
    "    :return: 注意力掩码\n",
    "    \"\"\"\n",
    "    # 设定subsequent_mask矩阵的shape\n",
    "    attn_shape = (1, size, size)\n",
    "    # 生成一个右上角(不含主对角线)为全1，左下角(含主对角线)为全0的subsequent_mask矩阵\n",
    "    subsequent_mask = np.triu(np.ones(attn_shape), k=1).astype('uint8')\n",
    "    # 返回一个右上角(不含主对角线)为全False，左下角(含主对角线)为全True的subsequent_mask矩阵\n",
    "    return torch.from_numpy(subsequent_mask) == 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97df4f92",
   "metadata": {},
   "source": [
    "## 5 词嵌入类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcaf8597",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    # 词嵌入层\n",
    "    def __init__(self, d_model, vocab):\n",
    "        \"\"\"\n",
    "        词嵌入层初始化\n",
    "        :param d_model: 词嵌入维度\n",
    "        :param vocab: 词表大小\n",
    "        \"\"\"\n",
    "        super(Embedding, self).__init__()\n",
    "        # Embedding层\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        # Embedding维数\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 返回x的词向量（需要乘以math.sqrt(d_model)）\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6fccb95",
   "metadata": {},
   "source": [
    "## 6 位置编码器类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0c071d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    # 位置编码器层\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000, device='cuda'):\n",
    "        \"\"\"\n",
    "        位置编码器层初始化\n",
    "        :param d_model: 词嵌入维度\n",
    "        :param dropout: dropout比例\n",
    "        :param max_len: 序列最大长度\n",
    "        :param device: 训练设备\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        # 位置编码矩阵，维度[max_len, embedding_dim]\n",
    "        pe = torch.zeros(max_len, d_model, device=device)\n",
    "        # 单词位置\n",
    "        position = torch.arange(0.0, max_len, device=device)\n",
    "        position.unsqueeze_(1)\n",
    "        # 使用exp和log实现幂运算\n",
    "        div_term = torch.exp(torch.arange(0.0, d_model, 2, device=device) * (- math.log(1e4) / d_model))\n",
    "        div_term.unsqueeze_(0)\n",
    "        # 计算单词位置沿词向量维度的纹理值\n",
    "        pe[:, 0:: 2] = torch.sin(torch.mm(position, div_term))\n",
    "        pe[:, 1:: 2] = torch.cos(torch.mm(position, div_term))\n",
    "        # 增加批次维度，[1, max_len, embedding_dim]\n",
    "        pe.unsqueeze_(0)\n",
    "        # 将位置编码矩阵注册为buffer(不参加训练)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 将一个批次中语句所有词向量与位置编码相加\n",
    "        # 注意，位置编码不参与训练，因此设置requires_grad=False\n",
    "        x += Variable(self.pe[:, : x.size(1), :], requires_grad=False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82d6f3a",
   "metadata": {},
   "source": [
    "## 7 多头注意力机制类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d0349f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    # 多头注意力机制\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"\"\"\n",
    "        多头注意力机制初始化\n",
    "        :param h: 多头\n",
    "        :param d_model: 词嵌入维度\n",
    "        :param dropout: dropout比例\n",
    "        \"\"\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        # 确保整除\n",
    "        assert d_model % h == 0\n",
    "        # q、k、v向量维数\n",
    "        self.d_k = d_model // h\n",
    "        # 头的数量\n",
    "        self.h = h\n",
    "        # WQ、WK、WV矩阵及多头注意力拼接变换矩阵WO 4个线性层\n",
    "        self.linears = nn.ModuleList([copy.deepcopy(nn.Linear(d_model, d_model)) for _ in range(4)])\n",
    "        # 注意力机制函数\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        反向传播\n",
    "        :param query: q\n",
    "        :param key: k\n",
    "        :param value: v\n",
    "        :param mask: 掩码\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "        # 批次大小\n",
    "        nbatches = query.size(0)\n",
    "        # WQ、WK、WV分别对词向量线性变换，并将结果拆成h块\n",
    "        query, key, value = [\n",
    "            l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "            for l, x in zip(self.linears, (query, key, value))\n",
    "        ]\n",
    "        # 注意力加权\n",
    "        x, self.attn = self.attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "        # 多头注意力加权拼接\n",
    "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)\n",
    "        # 对多头注意力加权拼接结果线性变换\n",
    "        return self.linears[-1](x)\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask=None, dropout=None):\n",
    "        \"\"\"\n",
    "        注意力加权\n",
    "        :param query: q\n",
    "        :param key: k\n",
    "        :param value: v\n",
    "        :param mask: 掩码矩阵\n",
    "        :param dropout: dropout比例\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # q、k、v向量长度为d_k\n",
    "        d_k = query.size(-1)\n",
    "        # 矩阵乘法实现q、k点积注意力，sqrt(d_k)归一化\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        # 注意力掩码机制\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        # 注意力矩阵softmax归一化\n",
    "        p_attn = F.softmax(scores, dim=-1)\n",
    "        # dropout\n",
    "        if dropout is not None:\n",
    "            p_attn = dropout(p_attn)\n",
    "        # 注意力对v加权\n",
    "        return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929c47ed",
   "metadata": {},
   "source": [
    "## 8 子层连接结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6de60f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    # 子层连接结构 用于连接注意力机制以及前馈全连接网络\n",
    "    def __init__(self, d_model, dropout):\n",
    "        \"\"\"\n",
    "        子层连接结构初始化层\n",
    "        :param d_model: 词嵌入纬度\n",
    "        :param dropout: dropout比例\n",
    "        \"\"\"\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        # 规范化层\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        # dropout层\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        # 层归一化\n",
    "        x_ = self.norm(x)\n",
    "        x_ = sublayer(x_)\n",
    "        x_ = self.dropout(x_)\n",
    "        # 残差连接\n",
    "        return x + x_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf100c0",
   "metadata": {},
   "source": [
    "## 9 前馈全连接网络类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5a692cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    # 前馈全连接网络\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        前馈全连接网络初始化层\n",
    "        :param d_model: 词嵌入维度\n",
    "        :param d_ff: 中间隐层维度\n",
    "        :param dropout: dropout比例\n",
    "        \"\"\"\n",
    "        super(FeedForward, self).__init__()\n",
    "        # 全连接层\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        # Dropout层\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.w_1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.w_2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b592b9ca",
   "metadata": {},
   "source": [
    "## 10 编码器类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "acb56fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    # 编码器\n",
    "    def __init__(self, h, d_model, d_ff=2048, dropout=0.1):\n",
    "        \"\"\"\n",
    "        编码器层初始化\n",
    "        :param h: 头数\n",
    "        :param d_model: 词嵌入维度\n",
    "        :param d_ff: 中间隐层维度\n",
    "        :param dropout: dropout比例\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        # 多头注意力\n",
    "        self.self_attn = MultiHeadedAttention(h, d_model)\n",
    "        # 前馈全连接层\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        # 子层连接结构\n",
    "        self.sublayer1 = SublayerConnection(d_model, dropout)\n",
    "        self.sublayer2 = SublayerConnection(d_model, dropout)\n",
    "        # 规范化层\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # 将embedding层进行Multi head Attention\n",
    "        x = self.sublayer1(x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        # attn的结果直接作为下一层输入\n",
    "        return self.norm(self.sublayer2(x, self.feed_forward))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82943b33",
   "metadata": {},
   "source": [
    "## 11 解码器类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "facdc3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, h, d_model, d_ff=2048, dropout=0.1):\n",
    "        \"\"\"\n",
    "        解码器层\n",
    "        :param h: 头数\n",
    "        :param d_model: 词嵌入维度\n",
    "        :param d_ff: 中间隐层维度\n",
    "        :param dropout: dropout比例\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.size = d_model\n",
    "        # 自注意力机制\n",
    "        self.self_attn = MultiHeadedAttention(h, d_model)\n",
    "        # 上下文注意力机制\n",
    "        self.src_attn = MultiHeadedAttention(h, d_model)\n",
    "        # 前馈全连接子层\n",
    "        self.feed_forward = FeedForward(d_model, d_ff, dropout)\n",
    "        # 子层连接结构\n",
    "        self.sublayer1 = SublayerConnection(d_model, dropout)\n",
    "        self.sublayer2 = SublayerConnection(d_model, dropout)\n",
    "        self.sublayer3 = SublayerConnection(d_model, dropout)\n",
    "        # 规范化层\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        # memory为编码器输出隐表示\n",
    "        m = memory\n",
    "        # 自注意力机制，q、k、v均来自解码器隐表示\n",
    "        x = self.sublayer1(x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        # 上下文注意力机制：q为来自解码器隐表示，而k、v为编码器隐表示\n",
    "        x = self.sublayer2(x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.norm(self.sublayer3(x, self.feed_forward))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8215038",
   "metadata": {},
   "source": [
    "## 12 生成器类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c09d0aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    #  生成器层\n",
    "    def __init__(self, d_model, vocab):\n",
    "        \"\"\"\n",
    "        生成器层初始化\n",
    "        :param d_model:\n",
    "        :param vocab:\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        # decode后的结果，先进入一个全连接层变为词典大小的向量\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 然后再进行log_softmax操作(在softmax结果上再做多一次log运算)\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d318d0eb",
   "metadata": {},
   "source": [
    "## 13 transformer框架"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00026630",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    # Transformer层\n",
    "    def __init__(self, tokenizer, h=8, d_model=256, E_N=2, D_N=2, device='cuda'):\n",
    "        \"\"\"\n",
    "        transformer层初始化\n",
    "        :param h: 头数\n",
    "        :param d_model: 词嵌入纬度\n",
    "        :param tokenizer:\n",
    "        :param E_N:\n",
    "        :param D_N:\n",
    "        :param device:\n",
    "        \"\"\"\n",
    "        super(Transformer, self).__init__()\n",
    "        # 编码器\n",
    "        self.encoder = nn.ModuleList([Encoder(h, d_model) for _ in range(E_N)])\n",
    "        # 解码器\n",
    "        self.decoder = nn.ModuleList([Decoder(h, d_model) for _ in range(D_N)])\n",
    "        # 词嵌入层\n",
    "        self.src_embed = Embedding(d_model, tokenizer.get_en_vocab_size())\n",
    "        self.tgt_embed = Embedding(d_model, tokenizer.get_ch_vocab_size())\n",
    "        # 位置编码器层\n",
    "        self.src_pos = PositionalEncoding(d_model, device=device)\n",
    "        self.tgt_pos = PositionalEncoding(d_model, device=device)\n",
    "        # 生成器层\n",
    "        self.generator = Generator(d_model, tokenizer.get_ch_vocab_size())\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        \"\"\"\n",
    "        编码\n",
    "        :param src: 源数据\n",
    "        :param src_mask: 源数据掩码\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # 词嵌入\n",
    "        src = self.src_embed(src)\n",
    "        # 位置编码\n",
    "        src = self.src_pos(src)\n",
    "        # 编码\n",
    "        for i in self.encoder:\n",
    "            src = i(src, src_mask)\n",
    "        return src\n",
    "\n",
    "    def decode(self, memory, tgt, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        解码\n",
    "        :param memory: 编码器输出\n",
    "        :param tgt: 目标数据输入\n",
    "        :param src_mask: 源数据掩码\n",
    "        :param tgt_mask: 目标数据掩码\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        #  词嵌入\n",
    "        tgt = self.tgt_embed(tgt)\n",
    "        #  位置编码\n",
    "        tgt = self.tgt_pos(tgt)\n",
    "        # 解码\n",
    "        for i in self.decoder:\n",
    "            tgt = i(tgt, memory, src_mask, tgt_mask)\n",
    "        return tgt\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        反向传播\n",
    "        :param src: 源数据\n",
    "        :param tgt: 目标数据\n",
    "        :param src_mask: 源数据掩码\n",
    "        :param tgt_mask: 目标数据掩码\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # encoder的结果作为decoder的memory参数传入，进行decode\n",
    "        return self.decode(self.encode(src, src_mask), tgt, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf8b6f2",
   "metadata": {},
   "source": [
    "## 14 标签平滑类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b02006b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    # 标签平滑\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        \"\"\"\n",
    "        初始化\n",
    "        :param size: 目标数据词表大小\n",
    "        :param padding_idx: 目标数据填充字符的索引\n",
    "        :param smoothing: 做平滑的值，为0即不进行平滑\n",
    "        \"\"\"\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        # KL散度，通常用于测量两个概率分布之间的差异\n",
    "        self.criterion = nn.KLDivLoss(reduction='sum')\n",
    "        # 目标数据填充字符的索引\n",
    "        self.padding_idx = padding_idx\n",
    "        # 置信度\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        # 平滑值\n",
    "        self.smoothing = smoothing\n",
    "        # 词表大小\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        \"\"\"\n",
    "        反向传播\n",
    "        :param x: 预测值\n",
    "        :param target: 目标值\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # 判断输出值的第二维传长度是否等于输出词表的大小，这里x的shape为 （batch*seqlength,x.shape(-1)）\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        # 标签平滑填充\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        # 这里的操作是将真实值的位置进行替换,替换成置信度\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        # 将填充的位置的值设置为0\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        # 生成填充部分的掩码\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        # 返回KL散度\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5ab2a5",
   "metadata": {},
   "source": [
    "## 15 损失计算类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2df0f21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLossCompute:\n",
    "    # 计算损失和进行参数反向传播\n",
    "    def __init__(self, generator, criterion, opt=None):\n",
    "        \"\"\"\n",
    "        初始化\n",
    "        :param generator: 生成器，transformer模块中的最后一层，这里将其单独拿出来而不直接放进transformer中的原因是：\n",
    "            预测数据的是时候，我们需要利用之前的结果，但是我们只去最后一个作为本次输出，那么在进行输出时，只对最后一个进行输出，单独拿出来进行输出的线性变换，更灵活\n",
    "        :param criterion: 标签平滑的类\n",
    "        :param opt: 经wormup后的optimizer\n",
    "        \"\"\"\n",
    "        self.generator = generator\n",
    "        self.criterion = criterion\n",
    "        self.opt = opt\n",
    "\n",
    "    def __call__(self, x, y, norm):\n",
    "        \"\"\"\n",
    "        类做函数调用\n",
    "        :param x: 经transformer解码后的结果\n",
    "        :param y: 目标值\n",
    "        :param norm: 本次数据有效的字符数，即，除去padding后的字符数\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # 进行输出\n",
    "        x = self.generator(x)\n",
    "        # 得到KL散度\n",
    "        loss = self.criterion(x.contiguous().view(-1, x.size(-1)),\n",
    "                              y.contiguous().view(-1)) / norm\n",
    "        # 反向椽笔\n",
    "        loss.backward()\n",
    "        if self.opt is not None:\n",
    "            # 参数更新\n",
    "            self.opt.step()\n",
    "            # 优化器梯度置0\n",
    "            self.opt.optimizer.zero_grad()\n",
    "        # 返回损失\n",
    "        return loss.data.item() * norm.float()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01a5bc1",
   "metadata": {},
   "source": [
    "## 16 Warmup-学习率更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c275e7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoamOpt:\n",
    "    # warmup\n",
    "    def __init__(self, model_size, factor, warmup, optimizer):\n",
    "        \"\"\"\n",
    "        初始化\n",
    "        :param model_size: 词嵌入维度\n",
    "        :param factor:\n",
    "        :param warmup:\n",
    "        :param optimizer:\n",
    "        \"\"\"\n",
    "        self.optimizer = optimizer\n",
    "        self._step = 0\n",
    "        self.warmup = warmup\n",
    "        self.factor = factor\n",
    "        self.model_size = model_size\n",
    "        self._rate = 0\n",
    "\n",
    "    def step(self):\n",
    "        # 学习率更新\n",
    "        self._step += 1\n",
    "        rate = self.rate()\n",
    "        for p in self.optimizer.param_groups:\n",
    "            p['lr'] = rate\n",
    "        self._rate = rate\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def rate(self, step=None):\n",
    "        # 学习率更新函数\n",
    "        if step is None:\n",
    "            step = self._step\n",
    "        return self.factor * (self.model_size ** (-0.5) * min(step ** (-0.5), step * self.warmup ** (-1.5)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb47ba2",
   "metadata": {},
   "source": [
    "## 17 训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c384beae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    en_path = r'training-parallel-nc-v13\\news-commentary-v13.zh-en.en'\n",
    "    ch_path = r'training-parallel-nc-v13\\news-commentary-v13.zh-en.zh'\n",
    "    tokenizer = classmodel.Tokenizer(en_path, ch_path, count_min=50)\n",
    "    device = 'cuda'\n",
    "    model = classmodel.Transformer(tokenizer)\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            # 这里初始化采用的是nn.init.xavier_uniform\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    model = model.to(device)\n",
    "    criteria = classmodel.LabelSmoothing(tokenizer.get_ch_vocab_size(), tokenizer.ch_word_2_index['<pad>'])\n",
    "    optimizer = classmodel.NoamOpt(256, 1, 2000,\n",
    "                                   torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))\n",
    "    lossF = classmodel.SimpleLossCompute(model.generator, criteria, optimizer)\n",
    "    epochs = 10\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        lod = 0\n",
    "        while True:\n",
    "            data_loader, p = tokenizer.get_dataloader()\n",
    "            if p:\n",
    "                break\n",
    "            for index, data in enumerate(data_loader):\n",
    "                # 生成数据\n",
    "                src, tgt = data\n",
    "                # 处理一个batch\n",
    "                batch = classmodel.Batch(src, tgt, tokenizer=tokenizer, device=device)\n",
    "                out = model(batch.src, batch.trg, batch.src_mask, batch.trg_mask)\n",
    "                out = lossF(out, batch.trg_y, batch.ntokens)\n",
    "                torch.cuda.empty_cache()\n",
    "                if index % 100 == 0:\n",
    "                    model.eval()\n",
    "                    print(epoch, lod, index, out / batch.ntokens)\n",
    "                    x = 'I have a dream!'\n",
    "                    # y = ['']\n",
    "                    x = tokenizer.en_encode(x)\n",
    "                    with torch.no_grad():\n",
    "                        predict(x, model, tokenizer)\n",
    "                    model.train()\n",
    "            lod += 1\n",
    "        torch.save(model.state_dict(), f'./model/translation_{epoch}.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298abc2a",
   "metadata": {},
   "source": [
    "## 18 预测函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af7ad45d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    \"\"\"\n",
    "    传入一个训练好的模型，对指定数据进行预测\n",
    "    \"\"\"\n",
    "    # 先用encoder进行encode\n",
    "    memory = model.encode(src, src_mask)\n",
    "    # 初始化预测内容为1×1的tensor，填入开始符('BOS')的id，并将type设置为输入数据类型(LongTensor)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    # 遍历输出的长度下标\n",
    "    for i in range(max_len - 1):\n",
    "        # decode得到隐层表示\n",
    "        out = model.decode(memory,\n",
    "                           Variable(ys),\n",
    "                           src_mask,\n",
    "                           Variable(subsequent_mask(ys.size(1)).type_as(src.data)))\n",
    "        # 将隐藏表示转为对词典各词的log_softmax概率分布表示\n",
    "        prob = model.generator(out[:, i])\n",
    "        # print('prob', prob)\n",
    "        # 获取当前位置最大概率的预测词id\n",
    "        _, next_word = torch.max(prob, dim=-1)\n",
    "        next_word = next_word.data[0]\n",
    "        # 将当前位置预测的字符id与之前的预测内容拼接起来\n",
    "        ys = torch.cat([ys,\n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "        # print(next_word)\n",
    "    return ys\n",
    "\n",
    "\n",
    "def predict(data, model, tokenizer, device='cuda'):\n",
    "    \"\"\"\n",
    "    在data上用训练好的模型进行预测，打印模型翻译结果\n",
    "    \"\"\"\n",
    "    # 梯度清零\n",
    "    with torch.no_grad():\n",
    "        # 在data的英文数据长度上遍历下标\n",
    "        for i in range(len(data)):\n",
    "            # 打印待翻译的英文语句\n",
    "\n",
    "            # 将当前以单词id表示的英文语句数据转为tensor，并放如DEVICE中\n",
    "            src = torch.from_numpy(np.array(data[i])).long().to(device)\n",
    "            # 增加一维\n",
    "            src = src.unsqueeze(0)\n",
    "            # 设置attention mask\n",
    "            src_mask = (src != tokenizer.en_word_2_index['<pad>']).unsqueeze(-2)\n",
    "            # 用训练好的模型进行decode预测\n",
    "            out = greedy_decode(model, src, src_mask, max_len=10, start_symbol=tokenizer.ch_word_2_index['<bos>'])\n",
    "            # 初始化一个用于存放模型翻译结果语句单词的列表\n",
    "            translation = []\n",
    "            # 遍历翻译输出字符的下标（注意：开始符\"BOS\"的索引0不遍历）\n",
    "            for j in range(1, out.size(1)):\n",
    "                # 获取当前下标的输出字符\n",
    "                sym = tokenizer.ch_index_2_word[out[0, j].item()]\n",
    "                # 如果输出字符不为'EOS'终止符，则添加到当前语句的翻译结果列表\n",
    "                if sym != '<eos>' and ' ':\n",
    "                    translation.append(sym)\n",
    "                # 否则终止遍历\n",
    "                else:\n",
    "                    break\n",
    "            # 打印模型翻译输出的中文语句结果\n",
    "            print(\"translation: %s\" % \" \".join(translation))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54989ea",
   "metadata": {},
   "source": [
    "## 19 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2496758e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英文词汇统计进度  0.03956056128524352\n",
      "英文词汇统计进度  0.07912112257048703\n",
      "英文词汇统计进度  0.11868168385573055\n",
      "英文词汇统计进度  0.15824224514097407\n",
      "英文词汇统计进度  0.1978028064262176\n",
      "英文词汇统计进度  0.2373633677114611\n",
      "英文词汇统计进度  0.2769239289967046\n",
      "英文词汇统计进度  0.31648449028194814\n",
      "英文词汇统计进度  0.35604505156719163\n",
      "英文词汇统计进度  0.3956056128524352\n",
      "英文词汇统计进度  0.43516617413767866\n",
      "英文词汇统计进度  0.4747267354229222\n",
      "英文词汇统计进度  0.5142872967081656\n",
      "英文词汇统计进度  0.5538478579934092\n",
      "英文词汇统计进度  0.5934084192786527\n",
      "英文词汇统计进度  0.6329689805638963\n",
      "英文词汇统计进度  0.6725295418491397\n",
      "英文词汇统计进度  0.7120901031343833\n",
      "英文词汇统计进度  0.7516506644196268\n",
      "英文词汇统计进度  0.7912112257048703\n",
      "英文词汇统计进度  0.8307717869901138\n",
      "英文词汇统计进度  0.8703323482753573\n",
      "英文词汇统计进度  0.9098929095606009\n",
      "英文词汇统计进度  0.9494534708458444\n",
      "英文词汇统计进度  0.9890140321310879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\30535\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.439 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中文词汇统计进度  0.03956056128524352\n",
      "中文词汇统计进度  0.07912112257048703\n",
      "中文词汇统计进度  0.11868168385573055\n",
      "中文词汇统计进度  0.15824224514097407\n",
      "中文词汇统计进度  0.1978028064262176\n",
      "中文词汇统计进度  0.2373633677114611\n",
      "中文词汇统计进度  0.2769239289967046\n",
      "中文词汇统计进度  0.31648449028194814\n",
      "中文词汇统计进度  0.35604505156719163\n",
      "中文词汇统计进度  0.3956056128524352\n",
      "中文词汇统计进度  0.43516617413767866\n",
      "中文词汇统计进度  0.4747267354229222\n",
      "中文词汇统计进度  0.5142872967081656\n",
      "中文词汇统计进度  0.5538478579934092\n",
      "中文词汇统计进度  0.5934084192786527\n",
      "中文词汇统计进度  0.6329689805638963\n",
      "中文词汇统计进度  0.6725295418491397\n",
      "中文词汇统计进度  0.7120901031343833\n",
      "中文词汇统计进度  0.7516506644196268\n",
      "中文词汇统计进度  0.7912112257048703\n",
      "中文词汇统计进度  0.8307717869901138\n",
      "中文词汇统计进度  0.8703323482753573\n",
      "中文词汇统计进度  0.9098929095606009\n",
      "中文词汇统计进度  0.9494534708458444\n",
      "中文词汇统计进度  0.9890140321310879\n",
      "432\n",
      "0 0 0 tensor(8.9762, device='cuda:0')\n",
      "translation: 保守 保守 或是 或是 不如 不如 不如 安全部队 安全部队\n",
      "0 0 100 tensor(8.2787, device='cuda:0')\n",
      "translation: 0 0 0 0 0 0 0 0 0\n",
      "0 0 200 tensor(6.8292, device='cuda:0')\n",
      "translation: 0 0 0 0 0 0 0 0 0\n",
      "0 0 300 tensor(6.3345, device='cuda:0')\n",
      "translation: 0 0 0 0 0 0 0 0 0\n",
      "0 1 0 tensor(6.3777, device='cuda:0')\n",
      "translation: 0 0 0 0 0 0 0 0 0\n",
      "0 1 100 tensor(6.1396, device='cuda:0')\n",
      "translation: 0 0 的 0 的 0 的 0 。\n",
      "0 1 200 tensor(6.0317, device='cuda:0')\n",
      "translation: 但 0 0 的 0 的 0 。\n",
      "0 1 300 tensor(5.6575, device='cuda:0')\n",
      "translation: 但 0 0 0 0 0 0 。\n",
      "0 2 0 tensor(5.9810, device='cuda:0')\n",
      "translation: 0 0 0 0 的 0 。\n",
      "0 2 100 tensor(5.6813, device='cuda:0')\n",
      "translation: 0 0 0 0 0 0 0 。\n",
      "0 2 200 tensor(5.6458, device='cuda:0')\n",
      "translation: 0 0 0 0 0 0 0 0 0\n",
      "0 2 300 tensor(5.6351, device='cuda:0')\n",
      "translation: 0 0 0 0 0 0 0 0 0\n",
      "0 3 0 tensor(5.9142, device='cuda:0')\n",
      "translation: 0 0 0 0 0 0 0 0 0\n",
      "0 3 100 tensor(5.4330, device='cuda:0')\n",
      "translation: 但 我 的 0 0 0 ， 他 的\n",
      "0 3 200 tensor(4.9600, device='cuda:0')\n",
      "translation: 我 的 0 0 0 0 0 0 0\n",
      "0 3 300 tensor(5.1457, device='cuda:0')\n",
      "translation: 我 的 我 我 的 0 0 ， 我\n",
      "0 4 0 tensor(5.2926, device='cuda:0')\n",
      "translation: 我 我 我 我 我 我 我 我 我\n",
      "0 4 100 tensor(5.1859, device='cuda:0')\n",
      "translation: 我 我 我 的 我 我 我 我 的\n",
      "0 4 200 tensor(5.1194, device='cuda:0')\n",
      "translation: 我 我 我 我 我 0 0 0 0\n",
      "0 4 300 tensor(5.0566, device='cuda:0')\n",
      "translation: 我 我 我 我 0 0 0 0 0\n",
      "0 5 0 tensor(5.1738, device='cuda:0')\n",
      "translation: 我 我 我 我 我 我 我 我 我\n",
      "0 5 100 tensor(4.9560, device='cuda:0')\n",
      "translation: 我 我 的 0 0 0 0 0 0\n",
      "0 5 200 tensor(4.7879, device='cuda:0')\n",
      "translation: 我 说 “ 我 我 我 我 的 0\n",
      "0 5 300 tensor(4.9100, device='cuda:0')\n",
      "translation: 我 我 我 我 我 我 我 我 我\n",
      "0 6 0 tensor(4.8459, device='cuda:0')\n",
      "translation: 我 我 我 我 我 我 我 0 0\n",
      "0 6 100 tensor(4.8785, device='cuda:0')\n",
      "translation: 我 0 0 0 0 0 我 0 0\n",
      "0 6 200 tensor(4.6615, device='cuda:0')\n",
      "translation: 我 0 我 0 我 0 我 0 我\n",
      "0 6 300 tensor(4.7076, device='cuda:0')\n",
      "translation: 我 说 我 说 我 0 0 0 0\n",
      "0 7 0 tensor(4.6199, device='cuda:0')\n",
      "translation: 我 0 0 0 0 0 0 0 0\n",
      "0 7 100 tensor(4.6039, device='cuda:0')\n",
      "translation: 我 我 说 我 我 我 我 的 0\n",
      "0 7 200 tensor(4.3729, device='cuda:0')\n",
      "translation: 我 0 0 0 0 0 0 0 0\n",
      "0 7 300 tensor(4.1672, device='cuda:0')\n",
      "translation: 我 认为 我 我 我 我 的 我 我\n",
      "0 8 0 tensor(4.4364, device='cuda:0')\n",
      "translation: 我 我 我 我 我 我 我 我 我\n",
      "0 8 100 tensor(4.3678, device='cuda:0')\n",
      "translation: 我 在 我 0 0 0 0 0 0\n",
      "0 8 200 tensor(4.4042, device='cuda:0')\n",
      "translation: 我 0 我 0 0 0 0 0 0\n",
      "0 8 300 tensor(4.2778, device='cuda:0')\n",
      "translation: 我 0 我 0 我 0 0 0 0\n",
      "0 9 0 tensor(4.4627, device='cuda:0')\n",
      "translation: 我 0 0 我 0 0 0 0 0\n",
      "0 9 100 tensor(4.3661, device='cuda:0')\n",
      "translation: 我 0 我 0 我 0 我 0 0\n",
      "0 9 200 tensor(4.3494, device='cuda:0')\n",
      "translation: 我 0 0 我 0 0 我 0 0\n",
      "0 9 300 tensor(4.3776, device='cuda:0')\n",
      "translation: 我 0 0 0 0 0 0 0 0\n",
      "0 10 0 tensor(4.5081, device='cuda:0')\n",
      "translation: 我 0 0 0 0 0 0 0 0\n",
      "0 10 100 tensor(4.5562, device='cuda:0')\n",
      "translation: 我 0 0 0 0 0 0 0 0\n",
      "0 10 200 tensor(4.1344, device='cuda:0')\n",
      "translation: 我 的 0 0 0 0 0 0 0\n",
      "0 10 300 tensor(4.1039, device='cuda:0')\n",
      "translation: 我 0 我 0 0 0 0 0 0\n",
      "0 11 0 tensor(4.3794, device='cuda:0')\n",
      "translation: 我 0 0 0 0 0 0 0 0\n",
      "0 11 100 tensor(4.2600, device='cuda:0')\n",
      "translation: 我 0 我 0 0 0 0 0 0\n",
      "0 11 200 tensor(4.1411, device='cuda:0')\n",
      "translation: 我 0 我 0 0 0 0 0 0\n",
      "0 11 300 tensor(4.3324, device='cuda:0')\n",
      "translation: 我 0 0 0 0 0 0 0 0\n",
      "0 12 0 tensor(4.1152, device='cuda:0')\n",
      "translation: 我 0 0 0 0 0 0 0 0\n",
      "0 12 100 tensor(4.1726, device='cuda:0')\n",
      "translation: 我 我 我 0 0 0 0 0 我\n",
      "0 12 200 tensor(4.0944, device='cuda:0')\n",
      "translation: 我 0 0 0 我 0 0 0 我\n",
      "0 12 300 tensor(3.7698, device='cuda:0')\n",
      "translation: 我 说 我 说 我 说 ， 我 说\n",
      "0 13 0 tensor(4.1460, device='cuda:0')\n",
      "translation: 我 0 我 0 0 0 0 0 0\n",
      "0 13 100 tensor(4.1422, device='cuda:0')\n",
      "translation: 我 我 0 我 0 0 我 0 0\n",
      "0 13 200 tensor(3.8882, device='cuda:0')\n",
      "translation: 我 0 0 0 0 0 0 0 0\n",
      "0 13 300 tensor(3.7873, device='cuda:0')\n",
      "translation: 我 0 我 0 我 的 0 0 我\n",
      "0 14 0 tensor(4.1456, device='cuda:0')\n",
      "translation: 我 0 我 0 我 0 0 0 我\n",
      "0 14 100 tensor(4.1082, device='cuda:0')\n",
      "translation: 我 0 我 的 0 我 0 0 0\n",
      "0 14 200 tensor(4.2018, device='cuda:0')\n",
      "translation: 我 0 我 的 0 0 0 0 0\n",
      "0 14 300 tensor(4.1497, device='cuda:0')\n",
      "translation: 我 0 我 的 0 我 0 我 0\n",
      "0 15 0 tensor(4.1759, device='cuda:0')\n",
      "translation: 我 希望 我 0 0 我 0 0 0\n",
      "0 15 100 tensor(4.0960, device='cuda:0')\n",
      "translation: 我 的 0 ！   我 0 ！\n",
      "0 15 200 tensor(3.8977, device='cuda:0')\n",
      "translation: 我 的 0 我 0 ！   我 的\n",
      "0 15 300 tensor(4.1796, device='cuda:0')\n",
      "translation: 我 的 0 ！   我 的 0 ！\n",
      "0 16 0 tensor(4.0431, device='cuda:0')\n",
      "translation: 我 的 0 ！   我 0 ！  \n",
      "0 16 100 tensor(3.8787, device='cuda:0')\n",
      "translation: 我 0 0 0 0 ！\n",
      "0 16 200 tensor(3.8229, device='cuda:0')\n",
      "translation: 我 的 0 ！\n",
      "0 16 300 tensor(3.9300, device='cuda:0')\n",
      "translation: 我 的 0 0 0 0 0 0 0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m train()\n",
      "Cell \u001b[1;32mIn[21], line 22\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m lod \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 22\u001b[0m     data_loader, p \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mget_dataloader()\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m p:\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[25], line 242\u001b[0m, in \u001b[0;36mTokenizer.get_dataloader\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03m获取dataloader\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;124;03m:return:\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;66;03m# 获取数据集\u001b[39;00m\n\u001b[1;32m--> 242\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_datasets()\n\u001b[0;32m    243\u001b[0m p \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;66;03m# 如果数据集为空，返回None\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[25], line 199\u001b[0m, in \u001b[0;36mTokenizer.__get_datasets\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    197\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# 将数据编码并\u001b[39;00m\n\u001b[1;32m--> 199\u001b[0m src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39men_encode(src)\n\u001b[0;32m    200\u001b[0m tgt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mch_encode(tgt)\n\u001b[0;32m    201\u001b[0m \u001b[38;5;66;03m# 返回数据集\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[25], line 143\u001b[0m, in \u001b[0;36mTokenizer.en_encode\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21men_encode\u001b[39m(\u001b[38;5;28mself\u001b[39m, data):\n\u001b[0;32m    138\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;124;03m    英文数据编码\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;124;03m    :param data: 需要编码的数据\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    :return: 返回编码后的数据集\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 143\u001b[0m     src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit_data(data, word_tokenize)\n\u001b[0;32m    144\u001b[0m     tokenized_data \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    145\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m src:\n\u001b[0;32m    146\u001b[0m         \u001b[38;5;66;03m# 用于存放每个句子对应的编码\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[25], line 133\u001b[0m, in \u001b[0;36mTokenizer.split_data\u001b[1;34m(self, data, func)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;66;03m# 对数据进行遍历\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;66;03m# 对数据进行分词\u001b[39;00m\n\u001b[1;32m--> 133\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m func(sentence)\n\u001b[0;32m    134\u001b[0m     tokens_data\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlist\u001b[39m(tokens))\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokens_data\n",
      "File \u001b[1;32mD:\\anac\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:130\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[1;32m--> 130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m ]\n",
      "File \u001b[1;32mD:\\anac\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:131\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;124;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;124;03m:type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m--> 131\u001b[0m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer\u001b[38;5;241m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m ]\n",
      "File \u001b[1;32mD:\\anac\\Lib\\site-packages\\nltk\\tokenize\\destructive.py:160\u001b[0m, in \u001b[0;36mNLTKWordTokenizer.tokenize\u001b[1;34m(self, text, convert_parentheses, return_str)\u001b[0m\n\u001b[0;32m    157\u001b[0m     text \u001b[38;5;241m=\u001b[39m regexp\u001b[38;5;241m.\u001b[39msub(substitution, text)\n\u001b[0;32m    159\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m regexp, substitution \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPUNCTUATION:\n\u001b[1;32m--> 160\u001b[0m     text \u001b[38;5;241m=\u001b[39m regexp\u001b[38;5;241m.\u001b[39msub(substitution, text)\n\u001b[0;32m    162\u001b[0m \u001b[38;5;66;03m# Handles parentheses.\u001b[39;00m\n\u001b[0;32m    163\u001b[0m regexp, substitution \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPARENS_BRACKETS\n",
      "File \u001b[1;32mD:\\anac\\Lib\\re\\__init__.py:322\u001b[0m, in \u001b[0;36m_subx.<locals>.filter\u001b[1;34m(match, template)\u001b[0m\n\u001b[0;32m    321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfilter\u001b[39m(match, template\u001b[38;5;241m=\u001b[39mtemplate):\n\u001b[1;32m--> 322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _parser\u001b[38;5;241m.\u001b[39mexpand_template(template, match)\n",
      "File \u001b[1;32mD:\\anac\\Lib\\re\\_parser.py:1097\u001b[0m, in \u001b[0;36mexpand_template\u001b[1;34m(template, match)\u001b[0m\n\u001b[0;32m   1095\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1096\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index, group \u001b[38;5;129;01min\u001b[39;00m groups:\n\u001b[1;32m-> 1097\u001b[0m         literals[index] \u001b[38;5;241m=\u001b[39m g(group) \u001b[38;5;129;01mor\u001b[39;00m empty\n\u001b[0;32m   1098\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m:\n\u001b[0;32m   1099\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minvalid group reference \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m index) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
